import dash
from dash import dcc, html, Input, Output
import dash_bootstrap_components as dbc
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_vgg
from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input as preprocess_densenet
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.sequence import pad_sequences
import base64
import io
from PIL import Image

# Load VGG16 model for feature extraction
vgg_model = VGG16(weights="imagenet")
vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)

# Load DenseNet model for feature extraction
densenet_model = DenseNet121(weights="imagenet")
densenet_model = Model(inputs=densenet_model.inputs, outputs=densenet_model.layers[-2].output)

# Load VGG caption generation model
vgg_caption_model = tf.keras.models.load_model("D:\\VIT\\BTech Sem 1\\Major_Project\\image-caption-generator-vgg\\Vgg_model.h5")

# Load DenseNet caption generation model
densenet_caption_model = tf.keras.models.load_model("D:\\VIT\\BTech Sem 1\\Major_Project\\image-caption-generator-denseNet\\DenseNetmodel.h5")

# Load tokenizers
with open(r"D:\\VIT\\BTech Sem 1\\Major_Project\\image-caption-generator-vgg\\tokenizer_vgg.pkl", 'rb') as file1:
    tokenizer_vgg = pickle.load(file1)
with open(r"D:\\VIT\\BTech Sem 1\\Major_Project\\image-caption-generator-denseNet\\tokenizer.pkl", 'rb') as file2:
    tokenizer_densenet = pickle.load(file2)

# Initialize Dash app
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
app.title = "Image Caption Generator - VGG and DenseNet"

# Define layout
app.layout = dbc.Container([
    dbc.Row(dbc.Col(html.H1("Image Caption Generator (VGG vs DenseNet)"), className="text-center")),
    dbc.Row(dbc.Col(html.P("Upload an image to compare captions generated by VGG and DenseNet models."), className="text-center")),
    dbc.Row([
        dbc.Col(dcc.Upload(
            id='upload-image',
            children=html.Div(['Drag and Drop or ', html.A('Select an Image')]),
            style={
                'width': '100%', 'height': '60px', 'lineHeight': '60px', 'borderWidth': '1px',
                'borderStyle': 'dashed', 'borderRadius': '5px', 'textAlign': 'center', 'margin': '10px'
            },
            multiple=False
        ), width=6),
    ], justify="center"),
    dbc.Row(dbc.Col(html.Div(id="output-image-upload"), className="text-center")),
    dbc.Row([
        dbc.Col(html.Div(id="vgg-caption", className="text-center"), width=6),
        dbc.Col(html.Div(id="densenet-caption", className="text-center"), width=6)
    ])
])

# Helper function to decode uploaded image
def decode_image(content):
    content_type, content_string = content.split(',')
    decoded = base64.b64decode(content_string)
    return Image.open(io.BytesIO(decoded))

# Helper function for generating a caption
def predict_caption(model, image_features, tokenizer, max_caption_length=37):
    caption = "startseq"
    for _ in range(max_caption_length):
        sequence = tokenizer.texts_to_sequences([caption])[0]
        sequence = pad_sequences([sequence], maxlen=max_caption_length)
        yhat = model.predict([image_features, sequence], verbose=0)
        predicted_index = np.argmax(yhat)
        predicted_word = tokenizer.index_word.get(predicted_index, None)
        if predicted_word is None or predicted_word == "endseq":
            break
        caption += " " + predicted_word
    return caption.replace("startseq", "").strip()

# Callback to process image and generate captions
@app.callback(
    [Output("output-image-upload", "children"),
     Output("vgg-caption", "children"),
     Output("densenet-caption", "children")],
    [Input("upload-image", "contents")]
)
def update_output(content):
    if content is not None:
        # Decode and preprocess the image
        image = decode_image(content)
        image = image.resize((224, 224))
        img_array = img_to_array(image)
        
        # Process for VGG
        img_vgg = preprocess_vgg(np.expand_dims(img_array, axis=0))
        vgg_features = vgg_model.predict(img_vgg)
        vgg_caption = predict_caption(vgg_caption_model, vgg_features, tokenizer_vgg)
        
        # Process for DenseNet
        img_densenet = preprocess_densenet(np.expand_dims(img_array, axis=0))
        densenet_features = densenet_model.predict(img_densenet)
        densenet_caption = predict_caption(densenet_caption_model, densenet_features, tokenizer_densenet)
        
        # Display results
        return [
            html.Img(src=content, style={'maxWidth': '100%', 'height': 'auto'}),
            html.Div(f'VGG Caption: "{vgg_caption}"', style={'font-style': 'italic', 'margin-top': '20px'}),
            html.Div(f'DenseNet Caption: "{densenet_caption}"', style={'font-style': 'italic', 'margin-top': '20px'})
        ]
    return [None, None, None]

# Run the app
if __name__ == "__main__":
    app.run_server(debug=True)
